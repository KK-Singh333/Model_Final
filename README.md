# ADformer: A Multi-Granularity Transformer for EEG-Based Alzheimerâ€™s Disease Assessment





### Processed data
The processed data should be put into `dataset/DATA_NAME/` so that each subject file can be located by `dataset/DATA_NAME/Feature/feature_ID.npy`, and the label file can be located by `dataset/DATA_NAME/Label/label.npy`.  

The processed datasets can be manually downloaded at the following links.
* ADSZ dataset: https://drive.google.com/file/d/1jkqawkx1v_vlf5zFOSgIh2LJs_OJMrvf/view?usp=drive_link
* APAVA dataset: https://drive.google.com/file/d/1ARjYLQ-tSEu0r3bffnLn3tCWdUiz4qXI/view?usp=drive_link
* ADFD dataset: https://drive.google.com/file/d/1ACYmO2ai9ad4C2zP3-qJFMjDpTexD321/view?usp=drive_link

Since other three datasets in the paper are private datasets, we do not provide a download link here. 



## Requirements  
  
The recommended requirements are specified as follows:  
* Python 3.8  
* Jupyter Notebook  
* einops==0.4.0
* matplotlib==3.7.0
* numpy==1.23.5
* pandas==1.5.3
* patool==1.12
* reformer-pytorch==1.4.4
* scikit-learn==1.2.2
* scipy==1.10.1
* sktime==0.16.1
* sympy==1.11.1
* torch==2.0.0
* tqdm==4.64.1
* wfdb==4.1.2
* neurokit2==0.2.9
* mne==1.6.1 
* natsort~=8.4.0

The dependencies can be installed by:  
```bash  
pip install -r requirements.txt
```



## Run Experiments
Before running, make sure you have all the processed datasets put under `dataset/`. For Linux users, run each method's shell script in `scripts/classification/`. 
You could also run all the experiments by running the `meta-run.py/` file, which the method included in _skip_list_ will not be run.
For Windows users, see jupyter notebook files. All the experimental scripts are provided cell by cell. 
The gpu device ids can be specified by setting command line `--devices` (e,g, `--devices 0,1,2,3`). 
You also need to change the visible gpu devices in script file by setting `export CUDA_VISIBLE_DEVICES` (e,g, `export CUDA_VISIBLE_DEVICES=0,1,2,3`). 
The gpu devices specified by commend line should be a subset of visible gpu devices.


After training and evaluation, the saved model can be found in`checkpoints/classification/`; 
and the results can be found in  `results/classification/`. 
You can modify the parameters by changing the command line. 
The meaning and explanation of each parameter in command line can be found in `run.py` file. 
Check the notebooks present in the folder, to the commands add following arguments also:-
--use_graph 1
--lambda1 <Value of first loss parameter>
--lambda2 <Value of second loss parameter>
The patch_len_lst and up_dim_lst must contain only single elements.
In features of final projection layer depend on the dataset, hence need to be changed for each dataset



## Acknowledgement

This codebase is constructed based on the repo: [Time-Series-Library](https://github.com/thuml/Time-Series-Library).
Thanks a lot for their amazing work on implementing state-of-arts time series methods!
